{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e87eaa",
   "metadata": {
    "id": "70e87eaa"
   },
   "source": [
    "# FNN Model Evaluation Notebook\n",
    "\n",
    "This notebook evaluates a Fully Connected Neural Network (FNN) model for predicting strain responses in layered elastic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc784cc6",
   "metadata": {
    "id": "cc784cc6"
   },
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, we install required packages and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uQMlEoAc086C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22335,
     "status": "ok",
     "timestamp": 1750132787972,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "uQMlEoAc086C",
    "outputId": "e5708c31-d4ae-4d3c-c138-9d38e62c6679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.13)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766b70a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18625,
     "status": "ok",
     "timestamp": 1750132812525,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "766b70a7",
    "outputId": "b35ed3bc-3e25-45a2-afb1-06618615f114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39277259-f20e-4f53-9681-4bf3023084ef",
   "metadata": {
    "executionInfo": {
     "elapsed": 27693,
     "status": "ok",
     "timestamp": 1750132860573,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "39277259-f20e-4f53-9681-4bf3023084ef"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import logging\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ddaf1",
   "metadata": {
    "id": "7a9ddaf1"
   },
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section handles the loading of pre-processed data required for model evaluation:\n",
    "\n",
    "- **batched_graph_test**: Contains the test dataset in graph format\n",
    "- **FrameLarge**: Contains the full dataset with material properties and responses\n",
    "- **Section**: Contains the layered structure information\n",
    "- **ZS_new**: Contains the z-coordinates (depth points) for each section\n",
    "- **xs**: Contains the x-coordinates (radial distance points)\n",
    "\n",
    "The data is loaded from pickle files stored in Google Drive, with paths specified in the configuration. Each dataset contains specific information about the pavement structure and its response to loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3Z3zuS1a8hLz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4995,
     "status": "ok",
     "timestamp": 1750132865592,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "3Z3zuS1a8hLz",
    "outputId": "b9201c95-9e8f-4c03-d777-a9b3ed5ee163"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d53d7a4d8247>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "def load_pickle(path):\n",
    "    \"\"\"Load data from pickle file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to pickle file\n",
    "\n",
    "    Returns:\n",
    "        Loaded data object\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "batched_graph_test_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/data/batched_graph_test.pkl'\n",
    "frame_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/data/frame_large.pkl'\n",
    "section_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/data/section.pkl'\n",
    "ZS_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/data/ZS.pkl'\n",
    "xs_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/data/xs.pkl'\n",
    "model_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/models/GAT_model.pt'\n",
    "\n",
    "\n",
    "batched_graph_test = load_pickle(batched_graph_test_path)\n",
    "FrameLarge = load_pickle(frame_path)\n",
    "Section = load_pickle(section_path)\n",
    "ZS_new = load_pickle(ZS_path)\n",
    "xs = load_pickle(xs_path)\n",
    "checkpoint = torch.load(model_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0134fcc",
   "metadata": {
    "id": "c0134fcc"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "This section defines three main configuration dictionaries that control the model and data generation:\n",
    "\n",
    "### Material Configuration (MATERIAL_CONFIG)\n",
    "- Defines properties of pavement materials (Asphalt Concrete, Base, Subgrade)\n",
    "- Specifies ranges for:\n",
    "  - Thickness (in inches)\n",
    "  - Modulus (in ksi)\n",
    "  - Poisson's ratio\n",
    "  - Number of sublayers\n",
    "- Sets increment steps for sampling material properties\n",
    "\n",
    "### Sampling Configuration (SAMPLING_CONFIG)\n",
    "- Controls data generation parameters:\n",
    "  - Number of points to generate\n",
    "  - Points along depth (z-axis) and radial distance (x-axis)\n",
    "  - Contact radius parameters\n",
    "  - Train/validation/test split indices\n",
    "  - Random seed for reproducibility\n",
    "\n",
    "### Model Configuration (MODEL_CONFIG)\n",
    "- Defines the GAT model architecture:\n",
    "  - Input dimension (5 features)\n",
    "  - Hidden layer dimensions (128, 90)\n",
    "  - Output dimension (3 strain components)\n",
    "  - Number of GAT layers (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3Cb0G53kzNLW",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1750132865622,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "3Cb0G53kzNLW"
   },
   "outputs": [],
   "source": [
    "MATERIAL_CONFIG = {\n",
    "    'N_MATERIALS': 3,\n",
    "    'MATERIAL_TYPES': ['AC', 'B', 'SG'],  # AC, base, subbase, subgrade\n",
    "    'SUBLAYER_MAX': [1, 1, 1],  # each material's max sublayers, excluding subgrade\n",
    "    'THICKNESS_RANGE': [[2, 16], [4, 20]],  # thickness range in inches\n",
    "    'MODULUS_RANGE': [[500, 2000], [50, 300], [5, 50]],  # modulus range in ksi\n",
    "    'THICKNESS_INCREMENT': [1, 2, 4],\n",
    "    'MODULUS_INCREMENT': [50, 20, 20, 5],  # increment in modulus sampling\n",
    "    'NU_RANGE': [[0.3, 0.4], [0.2, 0.499], [0.2, 0.499]]  # poissons ratio\n",
    "}\n",
    "\n",
    "SAMPLING_CONFIG = {\n",
    "    'N_POINTS': 1000,  # Number of points\n",
    "    'Z_POINTS': 14,  # points to generate along z\n",
    "    'X_POINTS': 10,  # points to generate along x\n",
    "    'A_RANGE': [4, 4],  # contact radius (in)\n",
    "    'A_POINTS': 1,  # how many contact radii to analyze\n",
    "    'FACTOR': 0.4,\n",
    "    'FILTER': 2,\n",
    "    'SPLIT_IDX': 800,\n",
    "    'TEST_IDX': 900,\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'INPUT_DIM': 5,\n",
    "    'HIDDEN_DIM1': 128,\n",
    "    'HIDDEN_DIM': 90,\n",
    "    'OUTPUT_DIM': 3,\n",
    "    'NUM_LAYERS': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70e58c7",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1750132865638,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "d70e58c7"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility across all random operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Random seed value\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed at the start\n",
    "set_seed(SAMPLING_CONFIG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629488c3",
   "metadata": {
    "id": "629488c3"
   },
   "source": [
    "## Data Generation Functions\n",
    "\n",
    "This section contains functions for generating and processing the layered elastic system data:\n",
    "\n",
    "### generatesection()\n",
    "Generates random pavement sections with realistic material properties:\n",
    "- Creates multiple layers with different materials\n",
    "- Assigns random but realistic values for:\n",
    "  - Layer thicknesses\n",
    "  - Modulus values\n",
    "  - Poisson's ratios\n",
    "- Handles sublayer generation based on thickness constraints\n",
    "- Returns both section properties and a structured DataFrame\n",
    "\n",
    "### generate_query_points()\n",
    "Generates the points where strain will be calculated:\n",
    "- Creates a grid of points in (x,z) space\n",
    "- Maps material properties to each point\n",
    "- Handles boundary conditions between layers\n",
    "- Returns query points and associated properties\n",
    "\n",
    "### remove_strain_z()\n",
    "Filters out sections with unrealistic strain values:\n",
    "- Removes sections where strain exceeds 1500 µε\n",
    "- Normalizes z-coordinates\n",
    "- Returns filtered dataset and updated z-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Zih3T0MNzoLs",
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1750132865727,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "Zih3T0MNzoLs"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generatesection(N,Nmaterial,MaterialType,Sublayermax,Thicknessrange,Modulusrange,\n",
    "                              zpoints,xpoints,Thicknessincrement,ModulusIncrement,nurange,arange,apoints):\n",
    "    \"\"\"Generate layered elastic system sections with random material properties.\n",
    "\n",
    "    Args:\n",
    "        N (int): Number of sections to generate\n",
    "        Nmaterial (int): Number of material types\n",
    "        MaterialType (list): List of material type names\n",
    "        Sublayermax (list): Maximum number of sublayers for each material\n",
    "        Thicknessrange (list): Thickness ranges for each material\n",
    "        Modulusrange (list): Modulus ranges for each material\n",
    "        zpoints (int): Number of points along depth\n",
    "        xpoints (int): Number of points along radial distance\n",
    "        Thicknessincrement (list): Increment steps for thickness\n",
    "        ModulusIncrement (list): Increment steps for modulus\n",
    "        nurange (list): Poisson's ratio ranges\n",
    "        arange (list): Contact radius range\n",
    "        apoints (int): Number of contact radii\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Section dictionary, DataFrame with section properties)\n",
    "    \"\"\"\n",
    "    Section={}\n",
    "    DataFrame={}\n",
    "    columns=['Structure','Pressure','ContactRadius','z','r']\n",
    "    thickCol=[f'H{i}' for i in range(1, sum(Sublayermax))]\n",
    "    columns.extend(thickCol)\n",
    "    modCol=[f'E{i}' for i in range(1, sum(Sublayermax)+1)]\n",
    "    columns.extend(modCol)\n",
    "    nuCol=[f'nu{i}' for i in range(1, sum(Sublayermax)+1)]\n",
    "    columns.extend(nuCol)\n",
    "\n",
    "    for sect in range(N):\n",
    "        Section[sect]={}\n",
    "        DataFrame[sect]=np.zeros(len(columns))\n",
    "        #To generate we have to follow some rules as described above\n",
    "        #First, decide on number of materials\n",
    "        MatNum=np.random.randint(2,Nmaterial+1)\n",
    "\n",
    "        #Then on the thickness\n",
    "        Thick=[]\n",
    "        Poisson=[]\n",
    "        Material=[]\n",
    "        #for each material, we will have poissons ratio and thickness\n",
    "\n",
    "\n",
    "        for i in range(MatNum-1):\n",
    "            T=np.arange(Thicknessrange[i][0],Thicknessrange[i][1]+Thicknessincrement[i],Thicknessincrement[i]) #Increments of 0.5in\n",
    "            P=np.arange(nurange[i][0],nurange[i][1]+0.001,0.05) #Increments of 0.05\n",
    "            Thick.append(np.random.choice(T))\n",
    "            Poisson.append(np.random.choice(P))\n",
    "            Material.append(MaterialType[i])\n",
    "        #For subgrade\n",
    "        P=np.arange(nurange[-1][0],nurange[-1][1]+0.001,0.05)\n",
    "        Poisson.append(np.random.choice(P))\n",
    "        Material.append(MaterialType[-1])\n",
    "\n",
    "        #Round the thickness and poissons ratio\n",
    "        Thick=np.round(Thick,2)\n",
    "        Poisson=np.round(Poisson,3)\n",
    "\n",
    "        #Now we can decide on sublayers\n",
    "        ThickSub=[]\n",
    "        MaterialSub=[]\n",
    "        ModulusSub=[]\n",
    "        PoissonSub=[]\n",
    "        for i in range(MatNum-1):\n",
    "            subs=np.random.randint(1,Sublayermax[i]+1)\n",
    "            M=np.arange(Modulusrange[i][0],Modulusrange[i][1]+ModulusIncrement[i],ModulusIncrement[i]) #increments of 50ksi\n",
    "            Modulus0=np.random.choice(M)\n",
    "\n",
    "            if Thick[i]/subs<1: #if smaller than 1 in, no sublayers\n",
    "                MaterialSub.append(MaterialType[i])\n",
    "                ThickSub.append(Thick[i])\n",
    "                ModulusSub.append(Modulus0)\n",
    "                PoissonSub.append(Poisson[i])\n",
    "\n",
    "                continue\n",
    "\n",
    "            for j in range(subs): #else, divide into sublayers\n",
    "                MaterialSub.append(MaterialType[i])\n",
    "                ThickSub.append(Thick[i]/subs)\n",
    "                PoissonSub.append(Poisson[i])\n",
    "\n",
    "                if j==0: #if we are at the first sublayer assign modulus 0\n",
    "                    ModulusSub.append(Modulus0)\n",
    "                else: #else, assign a smaller modulus\n",
    "                    Modulus0=np.random.uniform(low=Modulusrange[i][0], high=Modulus0)\n",
    "                    ModulusSub.append(Modulus0)\n",
    "\n",
    "        #For subgrade\n",
    "        ModulusSub.append(np.round(np.random.choice(np.arange(Modulusrange[-1][0],Modulusrange[-1][1]+ModulusIncrement[-1],ModulusIncrement[-1]))))\n",
    "        PoissonSub.append(Poisson[-1])\n",
    "        MaterialSub.append('SG')\n",
    "        #Round the values\n",
    "        ThickSub=np.round(ThickSub,2)\n",
    "        ModulusSub=np.round(ModulusSub)\n",
    "        PoissonSub=np.round(PoissonSub,3)\n",
    "\n",
    "        #To create the dictionary\n",
    "        Section[sect]['Material']=Material\n",
    "        Section[sect]['Thickness']=Thick\n",
    "        Section[sect]['Poisson']=Poisson\n",
    "        Section[sect]['MaterialSub']=MaterialSub\n",
    "        Section[sect]['ThicknessSub']=ThickSub\n",
    "        Section[sect]['PoissonSub']=PoissonSub\n",
    "        Section[sect]['ModulusSub']=ModulusSub\n",
    "\n",
    "        #To create the dataframe\n",
    "        t=np.append(Section[sect]['ThicknessSub'], np.zeros(sum(Sublayermax)-1-len(Section[sect]['ThicknessSub'])))\n",
    "        m=np.insert(Section[sect]['ModulusSub'], -1, np.zeros(sum(Sublayermax)-len(Section[sect]['ModulusSub'])))\n",
    "        p=np.insert(Section[sect]['PoissonSub'], -1, np.zeros(sum(Sublayermax)-len(Section[sect]['PoissonSub'])))\n",
    "        DataFrame[sect]=np.append(np.zeros(5),t)\n",
    "        DataFrame[sect][0]=sect+1 #assign structure\n",
    "        DataFrame[sect][1]=80 #assign pressure of 80 psi (9000/np.pi/6**2)\n",
    "        DataFrame[sect]=np.append(DataFrame[sect],m)\n",
    "        DataFrame[sect]=np.append(DataFrame[sect],p)\n",
    "    Frame=pd.DataFrame.from_dict(DataFrame, orient='index',columns=columns)\n",
    "    return Section,Frame\n",
    "\n",
    "def generate_query_points(Section, N,xpoints,zpoints,factor,arange,Frame):\n",
    "  \"\"\"Generate query points for strain calculation.\n",
    "\n",
    "    Args:\n",
    "        Section (dict): Section properties\n",
    "        N (int): Number of sections\n",
    "        xpoints (int): Number of x-points\n",
    "        zpoints (int): Number of z-points\n",
    "        factor (float): Sampling factor\n",
    "        arange (list): Contact radius range\n",
    "        Frame (DataFrame): Input frame with section data\n",
    "\n",
    "    Returns:\n",
    "        tuple: Various data structures for query points and properties\n",
    "  \"\"\"\n",
    "  FrameLarge_temp=[]\n",
    "  final_dict_ztoE=[]\n",
    "  final_dict_ztoH=[]\n",
    "  final_dict_ztonu = []\n",
    "  ZS=[]\n",
    "  H=[]\n",
    "  E=[]\n",
    "  NU = []\n",
    "\n",
    "  for i in range(N):\n",
    "    dict_z_to_E = {}\n",
    "    dict_z_to_H = {}\n",
    "    dict_z_to_nu={}\n",
    "    th=sum(Section[i]['Thickness'])+12\n",
    "    zs=np.power(np.linspace(np.sqrt(0.5),np.power(th,factor),zpoints),1/factor) #sampling near the surface\n",
    "    zs=np.sort(np.append(zs,np.append(np.cumsum(Section[i]['ThicknessSub'])+0.01,np.cumsum(Section[i]['ThicknessSub'])-0.01)))\n",
    "    ZS.append(zs)\n",
    "    E_per_section=np.zeros(len(zs))\n",
    "    H_per_section = np.zeros(len(zs))\n",
    "    nu_per_section = np.zeros(len(zs))\n",
    "    points_above_boundary=np.cumsum(Section[i]['ThicknessSub'])+0.01\n",
    "    points_below_boundary=np.cumsum(Section[i]['ThicknessSub'])-0.01\n",
    "    j=0\n",
    "\n",
    "    while j<=len(Section[i]['ModulusSub'])-1:\n",
    "      if j==0:\n",
    "        E_per_section[(zs<=points_below_boundary[j])]=Section[i]['ModulusSub'][j]\n",
    "        H_per_section[(zs<=points_below_boundary[j])]=Section[i]['ThicknessSub'][j]\n",
    "        nu_per_section[(zs<=points_below_boundary[j])]=Section[i]['PoissonSub'][j]\n",
    "        ind=0\n",
    "        j+=1\n",
    "      elif j>0 and j<len(Section[i]['ModulusSub'])-1:\n",
    "        E_per_section[(zs>=points_above_boundary[ind])&(zs<=points_below_boundary[j])]=Section[i]['ModulusSub'][j]\n",
    "        H_per_section[(zs>=points_above_boundary[ind])&(zs<=points_below_boundary[j])]=Section[i]['ThicknessSub'][j]\n",
    "        nu_per_section[(zs>=points_above_boundary[ind])&(zs<=points_below_boundary[j])]=Section[i]['PoissonSub'][j]\n",
    "        ind+=1\n",
    "        j+=1\n",
    "      elif j>=len(Section[i]['ModulusSub'])-1:\n",
    "        E_per_section[(zs>=points_above_boundary[ind])]=Section[i]['ModulusSub'][j]\n",
    "        H_per_section[(zs>=points_above_boundary[ind])]=-1\n",
    "        nu_per_section[(zs>=points_above_boundary[ind])]=Section[i]['PoissonSub'][j]\n",
    "        j+=1\n",
    "\n",
    "    for z,h in zip(zs,H_per_section):\n",
    "      dict_z_to_H[z] = h/100\n",
    "\n",
    "    final_dict_ztoH.append(dict_z_to_H)\n",
    "\n",
    "    for z,e in zip(zs,E_per_section):\n",
    "      dict_z_to_E[z]=e/100\n",
    "\n",
    "    final_dict_ztoE.append(dict_z_to_E)\n",
    "\n",
    "    for z,nu in zip(zs,nu_per_section):\n",
    "      dict_z_to_nu[z] = nu\n",
    "\n",
    "    final_dict_ztonu.append(dict_z_to_nu)\n",
    "\n",
    "    H.append(H_per_section)\n",
    "    E.append(E_per_section)\n",
    "    NU.append(nu_per_section)\n",
    "    xs = np.linspace(np.sqrt(0.5), np.sqrt(10), xpoints)**2\n",
    "    #radi=np.sort(random.sample(range(arange[0],arange[1]),apoints))\n",
    "    radi=[arange[0]]\n",
    "    Section[i]['z']=zs\n",
    "    Section[i]['x']=xs\n",
    "    Section[i]['a']=radi\n",
    "    FrameTemp=deepcopy(Frame.loc[i:i,:])\n",
    "    FrameTemp=pd.DataFrame(np.repeat(FrameTemp.values, len(radi)*len(zs)*len(xs), axis=0))\n",
    "    FrameTemp.columns = Frame.columns\n",
    "    res = np.matrix([[ii, j, k] for ii in radi\n",
    "                  for j in zs\n",
    "                  for k in xs])\n",
    "    FrameTemp.iloc[:,2:5]=res\n",
    "    FrameLarge_temp.append(FrameTemp)\n",
    "\n",
    "  FrameLarge_temp = pd.concat(FrameLarge_temp)\n",
    "  FrameLarge_temp=FrameLarge_temp.reset_index(drop=True)\n",
    "  FrameLarge_temp[['Displacement_Z', 'Displacement_H', 'Stress_Z', 'Stress_R', 'Stress_T', 'Stress_RZ', 'Strain_Z', 'Strain_R', 'Strain_T']]=0\n",
    "  return FrameLarge_temp, ZS, xs, E,NU,final_dict_ztoE,H,final_dict_ztoH,final_dict_ztonu\n",
    "\n",
    "def remove_strain_z(DF):\n",
    "    \"\"\"Remove sections with excessive strain values.\n",
    "\n",
    "    Args:\n",
    "        DF (DataFrame): Input dataframe with strain values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Filtered z-points, Filtered dataframe)\n",
    "    \"\"\"\n",
    "    ZS_new=[]\n",
    "    Length= DF[\"Structure\"].unique()\n",
    "\n",
    "    for structure in Length:\n",
    "        struct = int(structure) - 1\n",
    "        filtered = DF[DF[\"Structure\"] == structure]\n",
    "        inp = filtered.loc[:, [\"Strain_Z\"]] * 1e6\n",
    "        # Check if any Strain_Z value is greater than 2000\n",
    "        if (inp['Strain_Z'] >=1500).any():\n",
    "            DF = DF[DF[\"Structure\"] != structure]\n",
    "            continue\n",
    "        z_val = filtered['z'].unique()\n",
    "        ZS_new.append(z_val)\n",
    "\n",
    "    # normalizing\n",
    "    ZS_new=[ele/20 for ele in ZS_new]\n",
    "    return ZS_new,DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "SCxQO4CDy11l",
   "metadata": {
    "executionInfo": {
     "elapsed": 4566,
     "status": "ok",
     "timestamp": 1750132870295,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "SCxQO4CDy11l"
   },
   "outputs": [],
   "source": [
    "Section_temp, Frame = generatesection(\n",
    "        SAMPLING_CONFIG['N_POINTS'], MATERIAL_CONFIG['N_MATERIALS'],\n",
    "        MATERIAL_CONFIG['MATERIAL_TYPES'], MATERIAL_CONFIG['SUBLAYER_MAX'],\n",
    "        MATERIAL_CONFIG['THICKNESS_RANGE'], MATERIAL_CONFIG['MODULUS_RANGE'],\n",
    "        SAMPLING_CONFIG['Z_POINTS'], SAMPLING_CONFIG['X_POINTS'],\n",
    "        MATERIAL_CONFIG['THICKNESS_INCREMENT'], MATERIAL_CONFIG['MODULUS_INCREMENT'],\n",
    "        MATERIAL_CONFIG['NU_RANGE'], SAMPLING_CONFIG['A_RANGE'],\n",
    "        SAMPLING_CONFIG['A_POINTS']\n",
    ")\n",
    "\n",
    "FrameLarge_temp, ZS, xs, E, NU, final_dict_ztoE, H, final_dict_ztoH, final_dict_ztonu = generate_query_points(\n",
    "        Section, SAMPLING_CONFIG['N_POINTS'], SAMPLING_CONFIG['X_POINTS'],\n",
    "        SAMPLING_CONFIG['Z_POINTS'], SAMPLING_CONFIG['FACTOR'],\n",
    "        SAMPLING_CONFIG['A_RANGE'], Frame\n",
    ")\n",
    "ZS, DF = remove_strain_z(FrameLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ef4cdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1750133357052,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "d3ef4cdf",
    "outputId": "185af729-68e5-473c-e649-727aa1d5f04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in plot_dir plots, number 1\n"
     ]
    }
   ],
   "source": [
    "def convert_units(xs, zs):\n",
    "    xs_converted = xs * 2.54\n",
    "    ZS_converted = [i * 2.54 for i in zs]\n",
    "    xs = np.round(xs_converted, 2)\n",
    "    ZS_new = [np.round(zs, 2) for zs in ZS_converted]\n",
    "    return xs, ZS_new\n",
    "\n",
    "def build_pred_graph(batched_graph_test, final_y_pred):\n",
    "    pred_graph = {}\n",
    "    current_index = 0\n",
    "    for i in range(batched_graph_test.batch_size):\n",
    "        res_test = len(batched_graph_test[i].y)\n",
    "        pred_values = final_y_pred[current_index:current_index+res_test]\n",
    "        pred_graph[i] = pred_values\n",
    "        current_index += res_test\n",
    "    return pred_graph\n",
    "\n",
    "def setup_plotting():\n",
    "    # get unique number of experiment\n",
    "    plot_root = \"plots\"\n",
    "    if os.path.exists(plot_root):\n",
    "        avail_nums = os.listdir(plot_root)\n",
    "        avail_nums = [-1] + [int(d) for d in avail_nums if d.isdigit()]\n",
    "        exp_num = max(avail_nums) + 1\n",
    "    else:\n",
    "        exp_num = 0\n",
    "    exp_num = str(exp_num)\n",
    "    print(\"Logging in plot_dir {}, number {}\".format(plot_root, exp_num))\n",
    "\n",
    "\n",
    "    exp_plot_dir = os.path.join(plot_root, exp_num)\n",
    "\n",
    "    os.makedirs(exp_plot_dir, exist_ok=True)\n",
    "\n",
    "    log_path = os.path.join(plot_root, exp_num, \"log.txt\")\n",
    "    logging.basicConfig(filename=log_path,\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s | %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO, force=True)\n",
    "    logging.info(\"generating plots for experiment {}\".format(exp_num))\n",
    "\n",
    "    return exp_plot_dir\n",
    "\n",
    "plot_dir = setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed712bc",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Splitting\n",
    "\n",
    "This cell handles the preparation of the dataset for training, validation, and testing:\n",
    "\n",
    "- **Data Splitting:** The dataset is divided into three subsets:\n",
    "  - **Training set:** First 135,520 samples (80% of data)\n",
    "  - **Validation set:** Samples 135,520 to 152,720 (10% of data) \n",
    "  - **Test set:** Samples 152,720 to 169,799 (10% of data)\n",
    "\n",
    "- **Feature Selection:** \n",
    "  - **Input features (X):** Columns 3-13 (10 features) containing material properties and geometric parameters\n",
    "  - **Target variables (Y):** Columns 19-22 (3 features) representing strain components\n",
    "\n",
    "- **Data Standardization:** \n",
    "  - Both input features and target variables are standardized using `StandardScaler`\n",
    "  - This ensures all features have zero mean and unit variance, improving model training stability and convergence\n",
    "  - The scaler is fit on training data and applied consistently to validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "oipZqur5DGqO",
   "metadata": {
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1750133458307,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "oipZqur5DGqO"
   },
   "outputs": [],
   "source": [
    "x_train = DF.iloc[:135520, 3:13].values\n",
    "y_train = DF.iloc[:135520, 19:22].values\n",
    "x_val = DF.iloc[135520:152720, 3:13].values\n",
    "y_val = DF.iloc[135520:152720, 19:22].values\n",
    "x_test = DF.iloc[152720:169799, 3:13].values\n",
    "y_test = DF.iloc[152720:169799, 19:22].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scalery = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "y_train = scalery.fit_transform(y_train)\n",
    "y_val = scalery.transform(y_val)\n",
    "y_test = scalery.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab115f-b86a-4d98-8ae1-80a55ef0b587",
   "metadata": {
    "id": "a1ab115f-b86a-4d98-8ae1-80a55ef0b587"
   },
   "source": [
    "### FNNModel Class Description\n",
    "\n",
    "The `FNNModel` class implements a Feed-Forward Neural Network architecture for strain prediction in layered materials:\n",
    "\n",
    "- **Input Layer:** Accepts a configurable number of input features (typically 10 features from material and geometric properties)\n",
    "- **Hidden Layers:** Four fully connected layers, each with 64 neurons and ReLU activation functions\n",
    "  - `fc1`: Maps input to 64-dimensional hidden space\n",
    "  - `fc2`, `fc3`, `fc4`: Intermediate layers maintaining 64-dimensional representations\n",
    "  - `fc5`: Final layer producing 3 strain component outputs\n",
    "- **L1 Regularization:** Implements L1 penalty (Lasso regularization) with configurable lambda parameter (default: 1e-5) to prevent overfitting and promote feature sparsity\n",
    "- **Output Layer:** Produces 3 outputs corresponding to the three strain components (εz, εr, εθ)\n",
    "- **Purpose:** Designed for regression tasks, this architecture maps material properties and geometric parameters to strain responses in layered elastic systems\n",
    "\n",
    "The model uses L1 regularization to enhance generalization and prevent overfitting while maintaining computational efficiency for large-scale material analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15fdf672-41f8-4481-98ee-cecc3fd94892",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750133360432,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "15fdf672-41f8-4481-98ee-cecc3fd94892"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FNNModel(nn.Module):\n",
    "    def __init__(self, input_size: int, l1_lambda: float = 1e-5):\n",
    "        super(FNNModel, self).__init__()\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 3)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.act(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def l1_regularization(self) -> torch.Tensor:\n",
    "        l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
    "        return self.l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa98c7-5f58-4c6a-99dd-1c5133767296",
   "metadata": {
    "id": "40aa98c7-5f58-4c6a-99dd-1c5133767296"
   },
   "source": [
    " Load Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "zqoQgLZcDvJu",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750133391851,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "zqoQgLZcDvJu"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/models/FNN_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8843f2e4-373d-454a-bbeb-4d342caa76ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1750133392765,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "8843f2e4-373d-454a-bbeb-4d342caa76ce",
    "outputId": "47d9c914-a312-48fd-d1ef-29f2c4feaf71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-72495daa6031>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = x_test.shape[1]\n",
    "model = FNNModel(input_size)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a802253-d5ef-4ae9-8857-014d5408cf63",
   "metadata": {
    "id": "9a802253-d5ef-4ae9-8857-014d5408cf63"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This section evaluates the trained FNN model's performance:\n",
    "\n",
    "### Evaluation Process\n",
    "1. Loads the trained model checkpoint\n",
    "2. Runs inference on test data\n",
    "3. Calculates multiple metrics:\n",
    "   - Mean Squared Error (MSE) for each strain component\n",
    "   - Mean Absolute Error (MAE) for each strain component\n",
    "   - Mean Absolute Percentage Error (MAPE) for each strain component\n",
    "4. Logs results for analysis\n",
    "\n",
    "### Metrics Explanation\n",
    "- MSE: Measures average squared difference between predictions and actual values\n",
    "- MAE: Measures average absolute difference\n",
    "- MAPE: Measures relative error as a percentage\n",
    "- Each metric is calculated separately for:\n",
    "  - Vertical strain (εz)\n",
    "  - Radial strain (εr)\n",
    "  - Tangential strain (εθ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f0464b7-821b-454d-908e-00b8b4149071",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1750133472435,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "2f0464b7-821b-454d-908e-00b8b4149071",
    "outputId": "dfde91c0-eabc-4e76-d3c3-5d6e8d5ca918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in plot_dir plots, number 3\n",
      "MSE_Z: 614.5474897858791, MAE_Z: 6.318042938234566, MSE_R: 2.892334838615746, MAE_R: 0.9372502983024377, MSE_T: 3.312390884021762, MAE_T: 0.9982740003532877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.tensor(x_test,dtype=torch.float32)\n",
    "\n",
    "test_outputs = model(test_inputs)\n",
    "predicted_values = test_outputs.detach().numpy()\n",
    "y_pred=scalery.inverse_transform(predicted_values)\n",
    "y_plot=scalery.inverse_transform(y_test)\n",
    "final_y_plot = y_plot*1e6\n",
    "final_y_pred = y_pred*1e6\n",
    "\n",
    "plot_dir = setup_plotting()\n",
    "\n",
    "mse_z = mean_squared_error( final_y_plot[:, 0], final_y_pred[:, 0])\n",
    "mse_r = mean_squared_error( final_y_plot[:, 1], final_y_pred[:, 1])\n",
    "mse_t = mean_squared_error(  final_y_plot[:, 2], final_y_pred[:, 2])\n",
    "mae_z = mean_absolute_error( final_y_plot[:, 0], final_y_pred[:, 0])\n",
    "mae_r = mean_absolute_error(final_y_plot[:, 1], final_y_pred[:, 1])\n",
    "mae_t = mean_absolute_error(final_y_plot[:, 2], final_y_pred[:, 2])\n",
    "print(f\"MSE_Z: {mse_z}, MAE_Z: {mae_z}, MSE_R: {mse_r}, MAE_R: {mae_r}, MSE_T: {mse_t}, MAE_T: {mae_t}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefde7d",
   "metadata": {
    "id": "efefde7d"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "This section contains functions for visualizing model predictions and results:\n",
    "\n",
    "### plot_actual_vs_predicted_FNN()\n",
    "Creates scatter plots comparing actual vs predicted values:\n",
    "- Generates separate plots for each strain component\n",
    "- Includes perfect prediction line (y=x)\n",
    "- Uses consistent styling and labeling\n",
    "- Saves plots to specified directory\n",
    "\n",
    "### plot_heatmaps()\n",
    "Generates heatmaps of strain distributions:\n",
    "- Shows strain distribution in (x,z) space\n",
    "- Creates separate plots for:\n",
    "  - Actual strain values\n",
    "  - Predicted strain values\n",
    "- Includes:\n",
    "  - Color bars with strain values\n",
    "  - Proper axis labels and units\n",
    "  - Consistent styling and formatting\n",
    "- Saves high-resolution plots for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "FZEnQMUdo1QB",
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750133852708,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "FZEnQMUdo1QB"
   },
   "outputs": [],
   "source": [
    "plot_dir = '/content/drive/MyDrive/Graph_Neural_Network/PIML-main/plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b2532c",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1750133487735,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "c7b2532c"
   },
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(final_y_plot, final_y_pred, plot_dir=None):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    labels = ['z', 'r', 't']\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.scatter(final_y_plot[:, i], final_y_pred[:, i])\n",
    "        p1 = max(final_y_plot[:, i].max(), final_y_pred[:, i].max())\n",
    "        p2 = min(final_y_plot[:, i].min(), final_y_pred[:, i].min())\n",
    "        plt.plot([p2, p1], [p2, p1], color='black', linestyle='--')\n",
    "        plt.xlabel(f'Actual $\\\\mu\\\\epsilon_{label}$', fontsize=12)\n",
    "        plt.ylabel(f'Predicted $\\\\mu\\\\epsilon_{label}$', fontsize=12)\n",
    "        plt.title(f'Actual vs Predicted in $\\\\mu\\\\epsilon_{label}$', fontsize=12)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "        plt.tight_layout()\n",
    "        if plot_dir:\n",
    "            plt.savefig(os.path.join(plot_dir, f\"actual_vs_pred_{label}_fnn.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ade6c9-340f-4b8d-a105-4508ce1dcea1",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1750133514732,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "75ade6c9-340f-4b8d-a105-4508ce1dcea1"
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(ZS_new, xs, batched_graph_test, pred_graph, plot_dir=None, test_struct=0, test_g_struct=0):\n",
    "    # Strain_Z actual\n",
    "    sns.set_theme(rc={'figure.figsize':(30,20)}, font_scale=3)\n",
    "    z = np.array(ZS_new[test_struct])\n",
    "    response = 'Strain_Z'\n",
    "    A_prep = np.reshape(batched_graph_test[test_g_struct].y[:,0], (len(ZS_new[test_struct]), len(xs)))\n",
    "    heatmap = sns.heatmap(A_prep, linewidths=.5, xticklabels=xs, yticklabels=-z, cbar_kws={'label': 'Strain (µε)'})\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.ax.yaxis.label.set_size(30)\n",
    "    plt.xlabel('x (cm)', fontsize=30)\n",
    "    plt.ylabel('z (cm)', fontsize=30)\n",
    "    plt.title(response)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir:\n",
    "        plt.savefig(os.path.join(plot_dir, \"FNN_strain_z_heatmap_struct0_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Strain_Z predicted\n",
    "    sns.set_theme(rc={'figure.figsize':(20,10)}, font_scale=3)\n",
    "    A_bar = np.reshape(pred_graph[test_struct][:,0], (len(ZS_new[test_struct]), len(xs)))\n",
    "    heatmap = sns.heatmap(A_bar, linewidths=.5, xticklabels=xs, yticklabels=-z, cbar_kws={'label': 'Strain (µε)'})\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.ax.yaxis.label.set_size(30)\n",
    "    plt.xlabel('x (cm)', fontsize=30)\n",
    "    plt.ylabel('z (cm)', fontsize=30)\n",
    "    plt.title('$\\\\epsilon_z$')\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir:\n",
    "        plt.savefig(os.path.join(plot_dir, \"FNN_strain_z_heatmap_struct0_pred.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Strain_R actual\n",
    "    sns.set_theme(rc={'figure.figsize':(30,20)}, font_scale=3)\n",
    "    response = 'Strain_R'\n",
    "    A_prep = np.reshape(batched_graph_test[test_g_struct].y[:,1], (len(ZS_new[test_struct]), len(xs)))\n",
    "    heatmap = sns.heatmap(A_prep, linewidths=.5, xticklabels=xs, yticklabels=-z, cbar_kws={'label': 'Strain (µε)'})\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.ax.yaxis.label.set_size(30)\n",
    "    plt.xlabel('x (cm)', fontsize=30)\n",
    "    plt.ylabel('z (cm)', fontsize=30)\n",
    "    plt.title(response)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir:\n",
    "        plt.savefig(os.path.join(plot_dir, \"FNN_strain_r_heatmap_struct0_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Strain_R predicted\n",
    "    sns.set_theme(rc={'figure.figsize':(20,10)}, font_scale=3)\n",
    "    A_bar = np.reshape(pred_graph[test_struct][:,1], (len(ZS_new[test_struct]), len(xs)))\n",
    "    heatmap = sns.heatmap(A_bar, linewidths=.5, xticklabels=xs, yticklabels=-z, cbar_kws={'label': 'Strain (µε)'})\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.ax.yaxis.label.set_size(30)\n",
    "    plt.xlabel('x (cm)', fontsize=30)\n",
    "    plt.ylabel('z (cm)', fontsize=30)\n",
    "    plt.title('$\\\\epsilon_r$')\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir:\n",
    "        plt.savefig(os.path.join(plot_dir, \"FNN_strain_r_heatmap_struct0_pred.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82bdb35e",
   "metadata": {
    "executionInfo": {
     "elapsed": 7474,
     "status": "ok",
     "timestamp": 1750133912297,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "82bdb35e"
   },
   "outputs": [],
   "source": [
    "# Convert units\n",
    "xs, ZS_new = convert_units(xs, ZS_new)\n",
    "\n",
    "# Build prediction graph\n",
    "pred_graph = build_pred_graph(batched_graph_test, predicted_values)\n",
    "\n",
    "# Plot heatmaps\n",
    "plot_heatmaps(ZS_new, xs, batched_graph_test, pred_graph, plot_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "u5KTARD9nip0",
   "metadata": {
    "executionInfo": {
     "elapsed": 6298,
     "status": "ok",
     "timestamp": 1750133957829,
     "user": {
      "displayName": "Urja Giridharan",
      "userId": "03202406529544037479"
     },
     "user_tz": 240
    },
    "id": "u5KTARD9nip0"
   },
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(final_y_plot, final_y_pred, plot_dir=plot_dir)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1QXBGDEgXzHIziuW4fRaxW_6mYaMjno6p",
     "timestamp": 1750132617474
    }
   ]
  },
  "kernelspec": {
   "display_name": "gnn_kernel",
   "language": "python",
   "name": "gnn_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
